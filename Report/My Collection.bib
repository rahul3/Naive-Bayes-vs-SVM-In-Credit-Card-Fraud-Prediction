@article{Chawla2002,
author = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/Synthetic Minority Oversampling Technique.pdf:pdf},
pages = {321--357},
title = {{SMOTE : Synthetic Minority Over-sampling TEchnique}},
volume = {16},
year = {2002}
}
@article{Akbani2004,
author = {Akbani, Rehan and Kwek, Stephen and Japkowicz, Nathalie},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/Applying Support Vector Machines to Imbalanced Datasets.pdf:pdf},
pages = {39--50},
title = {{to Imbalanced Datasets}},
year = {2004}
}
@article{Chang2013,
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Chang, Chih-chung and Lin, Chih-jen},
doi = {10.1145/1961189.1961199},
eprint = {0-387-31073-8},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/libsvm - A Library For Support Vector Machines.pdf:pdf},
isbn = {2157-6904},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
keywords = {classification,libsvm,optimization,regression,support vector ma-},
pages = {1--39},
pmid = {371},
title = {{LIBSVM : A Library for Support Vector Machines}},
volume = {2},
year = {2013}
}
@article{Langley1995,
archivePrefix = {arXiv},
arxivId = {1302.4964},
author = {Langley, Pat and John, Geroge H.},
doi = {10.1.1.8.3257},
eprint = {1302.4964},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/Estimating Continuous Distributions in Bayesian Classifiers.pdf:pdf},
isbn = {1-55860-385-9},
issn = {1558603859},
journal = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
pages = {399--406},
pmid = {1000253971},
title = {{Estimating continuous distributions in Bayesian classifier}},
year = {1995}
}
@article{Moore2005,
abstract = {Accurate traffic classification is of fundamental importance to numerous other network activities, from security monitoring to accounting, and from Quality of Service to providing operators with useful forecasts for long-term provisioning. We apply a Na{\"{i}}ve Bayes estimator to categorize traffic by application. Uniquely, our work capitalizes on hand-classified network data, using it as input to a supervised Na{\"{i}}ve Bayes estimator. In this paper we illustrate the high level of accuracy achievable with the $\backslash$Naive Bayes estimator. We further illustrate the improved accuracy of refined variants of this estimator.Our results indicate that with the simplest of Na{\"{i}}ve Bayes estimator we are able to achieve about 65{\%} accuracy on per-flow classification and with two powerful refinements we can improve this value to better than 95{\%}; this is a vast improvement over traditional techniques that achieve 50--70{\%}. While our technique uses training data, with categories derived from packet-content, all of our training and testing was done using header-derived discriminators. We emphasize this as a powerful aspect of our approach: using samples of well-known traffic to allow the categorization of traffic using commonly available information alone.},
author = {Moore, Andrew W. and Zuev, Denis and Moore, Andrew W. and Zuev, Denis},
doi = {10.1145/1064212.1064220},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/Internet Traffic Classification Using Bayesian Analysis Techniques.pdf:pdf},
isbn = {1595930221},
issn = {01635999},
journal = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems - SIGMETRICS '05},
keywords = {andrew moore thanks the,fellowship,flow classification,generous sup-,intel corporation for its,internet traffic,port of his research,traffic identification},
number = {1},
pages = {50},
title = {{Internet traffic classification using bayesian analysis techniques}},
url = {http://portal.acm.org/citation.cfm?doid=1064212.1064220},
volume = {33},
year = {2005}
}
@article{Perez2009,
abstract = {When learning Bayesian network based classifiers continuous variables are usually handled by discretization, or assumed that they follow a Gaussian distribution. This work introduces the kernel based Bayesian network paradigm for supervised classification. This paradigm is a Bayesian network which estimates the true density of the continuous variables using kernels. Besides, tree-augmented naive Bayes, k-dependence Bayesian classifier and complete graph classifier are adapted to the novel kernel based Bayesian network paradigm. Moreover, the strong consistency properties of the presented classifiers are proved and an estimator of the mutual information based on kernels is presented. The classifiers presented in this work can be seen as the natural extension of the flexible naive Bayes classifier proposed by John and Langley [G.H. John, P. Langley, Estimating continuous distributions in Bayesian classifiers, in: Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, 1995, pp. 338-345], breaking with its strong independence assumption. Flexible tree-augmented naive Bayes seems to have superior behavior for supervised classification among the flexible classifiers. Besides, flexible classifiers presented have obtained competitive errors compared with the state-of-the-art classifiers. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {P{\'{e}}rez, Aritz and Larra{\~{n}}aga, Pedro and Inza, I{\~{n}}aki},
doi = {10.1016/j.ijar.2008.08.008},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/Bayesian Classifers Based on KDF.pdf:pdf},
isbn = {0888-613X},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Bayesian network,Flexible naive Bayes,Kernel density estimation,Supervised classification},
number = {2},
pages = {341--362},
publisher = {Elsevier Inc.},
title = {{Bayesian classifiers based on kernel density estimation: Flexible classifiers}},
url = {http://dx.doi.org/10.1016/j.ijar.2008.08.008},
volume = {50},
year = {2009}
}
@article{Srivastava2008,
author = {Srivastava, A. and Kundu, A. and Sural, S. and Majumdar, A.K.},
file = {:Users/rahul{\_}padmanabhan/Documents/Data Mining Class/Project/Reference/Credit Card Fraud.pdf:pdf},
isbn = {3060296103},
issn = {1545-5971},
journal = {IEEE Transactions on Dependable and Secure Computing},
keywords = {fraud detection,payment system,self organizing map,transaction},
number = {1},
pages = {37--48},
title = {{Credit Card Fraud Detection Using self}},
url = {http://www.procon.bg/node/1477{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4358713},
volume = {5},
year = {2008}
}
